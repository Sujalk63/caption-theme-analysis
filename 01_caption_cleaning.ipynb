{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6f634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import regex as re \n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4074b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sujal Karmakar\\Desktop\\Desktop\\Data Analyst\\Python\\python_data_analytics_project\\Theme Finder Using Caption (NLP)\\Data\\leanbeast_analysis_ready.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4673a4",
   "metadata": {},
   "source": [
    "#### 1) Basic Cleaning (lower case, remover numbers, emoji, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b6d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caption = df[\"caption_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b5473a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean each caption: lowercase, remove punctuation, then remove emojis\n",
    "def clean_caption(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()  # Lowercase\n",
    "\n",
    "    text = re.sub(r\"\\p{P}+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001f600-\\U0001f64f\"  # emoticons\n",
    "        \"\\U0001f300-\\U0001f5ff\"  # symbols & pictographs\n",
    "        \"\\U0001f680-\\U0001f6ff\"  # transport & map symbols\n",
    "        \"\\U0001f1e0-\\U0001f1ff\"  # flags (iOS)\n",
    "        \"\\U00002700-\\U000027bf\"  # Dingbats\n",
    "        \"\\U0001f900-\\U0001f9ff\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U00002600-\\U000026ff\"  # Misc symbols\n",
    "        \"\\U00002b00-\\U00002bff\"  # Misc symbols and arrows\n",
    "        \"\\U0001fa70-\\U0001faff\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\ufe0f\"  # Variation Selector-16 (emoji style)\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    text = emoji_pattern.sub(r\"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply to the caption column\n",
    "df_caption_cleaned = df_caption.apply(clean_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b59a61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947                                        iamsrk ka diet\n",
       "309                                    mere ghar ka khana\n",
       "1032    vacation look in   7 days 7 different outfits ...\n",
       "299     comment snacks and i will send you 5 healthy s...\n",
       "349                                  best fatloss machine\n",
       "250     taking an ice bath cold water immersion offers...\n",
       "1031    gym membership if you agree do comment like an...\n",
       "1024                                          hair hairs \n",
       "1440    manish bhai lost 13 kgs in 3 months and is now...\n",
       "1073    well done darshilsangani my client from canada...\n",
       "351                                                raisin\n",
       "1013                           hydration is imp stay safe\n",
       "407     comment berry  to get original blueberries buy...\n",
       "1182    if you are skinny fat do this  abs fitness gym...\n",
       "1077                     fake supplements exposed zeenews\n",
       "Name: caption_text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caption_cleaned.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd25b2",
   "metadata": {},
   "source": [
    "#### 2) Tokenization (removing stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6e2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# # Define a custom download directory\n",
    "# custom_nltk_path = r'C:\\Users\\Sujal Karmakar\\.conda\\envs\\DS\\nltk_data'\n",
    "\n",
    "# # Download resources to the custom path\n",
    "## nltk.download('punkt', download_dir=custom_nltk_path)\n",
    "# nltk.download('punkt_tab', download_dir=custom_nltk_path)\n",
    "# nltk.download('stopwords', download_dir=custom_nltk_path)\n",
    "\n",
    "# # Manually add the path to NLTK's search locations\n",
    "# nltk.data.path.append(custom_nltk_path)\n",
    "\n",
    "# i had to explicitly download these by specifying files and also punkt was not working for me nltk was demanding punkt_tab so i did that \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87b9e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [well, done, jaibajaj0786, lost, 39kgs, withou...\n",
      "1    [transformation, alert, keen, interested, paid...\n",
      "2    [shredded, mode, manavkansagra1111, guidance, ...\n",
      "3    [comment, atta, send, link, healthy, atta, mes...\n",
      "4    [dinner, date, couple, love, couplegoals, wedd...\n",
      "Name: caption_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Set of English stopwords for filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_caption(text):\n",
    "    # Handle non-string inputs safely\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # Step 1: Sentence tokenize the text\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Step 2: Word tokenize each sentence\n",
    "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Step 3: Flatten the list of token lists into a single list\n",
    "    flat_tokens = [token for sublist in tokens for token in sublist]\n",
    "\n",
    "    # Step 4: Remove stopwords (case-insensitive)\n",
    "    filtered_tokens = [w for w in flat_tokens if w.lower() not in stop_words]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "df_caption_cleaned_tokens = df_caption_cleaned.apply(preprocess_caption)\n",
    "\n",
    "# Show first few results\n",
    "print(df_caption_cleaned_tokens.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59549472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       well done jaibajaj0786 lost 39kgs without coun...\n",
       "1       transformation alert   if he keen do it  why n...\n",
       "2       shredded mode on manavkansagra1111 under my gu...\n",
       "3       comment atta and i will send you the link of h...\n",
       "4       dinner date   couple love couplegoals wedding ...\n",
       "                              ...                        \n",
       "1546    lockdown transformation lost 15 kgs in 70 days...\n",
       "1547    shoot mode  modelifefitness gym workout motiva...\n",
       "1548    manage your life  office and work  paid online...\n",
       "1549    throwback to one my client who reduced 38 kgs ...\n",
       "1550    taj mahal is beautiful today gymlife training ...\n",
       "Name: caption_text, Length: 1551, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caption_combined = pd.DataFrame({\n",
    "    \"cleaned_caption\": df_caption_cleaned,\n",
    "    \"tokens\": df_caption_cleaned_tokens\n",
    "})\n",
    "df_caption_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70213a3f",
   "metadata": {},
   "source": [
    "#### 3) Creating dictionaries and corpus for LDA (Linear discriminant analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a7289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 677\n",
      "Number of documents: 1551\n",
      "Sample doc (bow format): [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Convert the Series of lists into a list of lists\n",
    "texts = df_caption_cleaned_tokens.tolist()\n",
    "\n",
    "# Create Dictionary: maps each word to a unique id\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Filter extremes to remove very rare and very common words (optional but recommended)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)  # keeps words in at least 5 docs but less than 50% of all docs\n",
    "\n",
    "# Create Corpus: list of bags of words for each document\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Print some info to confirm\n",
    "print(f\"Number of unique tokens: {len(dictionary)}\")\n",
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "print(f\"Sample doc (bow format): {corpus[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6d17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
