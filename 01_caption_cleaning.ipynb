{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d6f634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import regex as re \n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4074b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sujal Karmakar\\Desktop\\Desktop\\Data Analyst\\Python\\python_data_analytics_project\\Theme Finder Using Caption (NLP)\\Data\\leanbeast_analysis_ready.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4673a4",
   "metadata": {},
   "source": [
    "#### 1) Basic Cleaning (lower case, remover numbers, emoji, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5b6d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caption = df[\"caption_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b5473a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean each caption: lowercase, remove punctuation, then remove emojis\n",
    "def clean_caption(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()  # Lowercase\n",
    "\n",
    "    text = re.sub(r\"\\p{P}+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001f600-\\U0001f64f\"  # emoticons\n",
    "        \"\\U0001f300-\\U0001f5ff\"  # symbols & pictographs\n",
    "        \"\\U0001f680-\\U0001f6ff\"  # transport & map symbols\n",
    "        \"\\U0001f1e0-\\U0001f1ff\"  # flags (iOS)\n",
    "        \"\\U00002700-\\U000027bf\"  # Dingbats\n",
    "        \"\\U0001f900-\\U0001f9ff\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U00002600-\\U000026ff\"  # Misc symbols\n",
    "        \"\\U00002b00-\\U00002bff\"  # Misc symbols and arrows\n",
    "        \"\\U0001fa70-\\U0001faff\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\ufe0f\"  # Variation Selector-16 (emoji style)\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    text = emoji_pattern.sub(r\"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply to the caption column\n",
    "df_caption_cleaned = df_caption.apply(clean_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b59a61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802     lets see if my inspiration srk comments   iams...\n",
       "1535    same garment same person  transformation 2016 ...\n",
       "1439    beauty and the beast  couple love couplegoals ...\n",
       "834                                               aree bc\n",
       "887                                              new look\n",
       "830                                 clue healthiest snack\n",
       "1505            happy one month wedding anniversary love \n",
       "750     greece ka sapna manali meh pura hua  went shir...\n",
       "1121    kya bhai log kya kya samajh jaate hai  abs fit...\n",
       "167     comment testosterone and i will send you a blu...\n",
       "993     how i handled all challenges of my life after ...\n",
       "1467                                    pov beast husband\n",
       "745      itna paani peena zaruri hai for better hydration\n",
       "481     katarzyna jakubowska poland has become the fir...\n",
       "290     how i will consume 250gm of protein after 26 h...\n",
       "Name: caption_text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caption_cleaned.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd25b2",
   "metadata": {},
   "source": [
    "#### 2) Tokenization (removing stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e6e2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# # Define a custom download directory\n",
    "# custom_nltk_path = r'C:\\Users\\Sujal Karmakar\\.conda\\envs\\DS\\nltk_data'\n",
    "\n",
    "# # Download resources to the custom path\n",
    "## nltk.download('punkt', download_dir=custom_nltk_path)\n",
    "# nltk.download('punkt_tab', download_dir=custom_nltk_path)\n",
    "# nltk.download('stopwords', download_dir=custom_nltk_path)\n",
    "\n",
    "# # Manually add the path to NLTK's search locations\n",
    "# nltk.data.path.append(custom_nltk_path)\n",
    "\n",
    "# i had to explicitly download these by specifying files and also punkt was not working for me nltk was demanding punkt_tab so i did that \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b87b9e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [well, done, jaibajaj0786, lost, 39kgs, withou...\n",
      "1    [transformation, alert, keen, interested, paid...\n",
      "2    [shredded, mode, manavkansagra1111, guidance, ...\n",
      "3    [comment, atta, send, link, healthy, atta, mes...\n",
      "4    [dinner, date, couple, love, couplegoals, wedd...\n",
      "Name: caption_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Set of English stopwords for filtering\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_caption(text):\n",
    "    # Handle non-string inputs safely\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # Step 1: Sentence tokenize the text\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Step 2: Word tokenize each sentence\n",
    "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Step 3: Flatten the list of token lists into a single list\n",
    "    flat_tokens = [token for sublist in tokens for token in sublist]\n",
    "\n",
    "    # Step 4: Remove stopwords (case-insensitive)\n",
    "    filtered_tokens = [w for w in flat_tokens if w.lower() not in stop_words]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "df_caption_cleaned_tokens = df_caption_cleaned.apply(preprocess_caption)\n",
    "\n",
    "# Show first few results\n",
    "print(df_caption_cleaned_tokens.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59549472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       well done jaibajaj0786 lost 39kgs without coun...\n",
       "1       transformation alert   if he keen do it  why n...\n",
       "2       shredded mode on manavkansagra1111 under my gu...\n",
       "3       comment atta and i will send you the link of h...\n",
       "4       dinner date   couple love couplegoals wedding ...\n",
       "                              ...                        \n",
       "1546    lockdown transformation lost 15 kgs in 70 days...\n",
       "1547    shoot mode  modelifefitness gym workout motiva...\n",
       "1548    manage your life  office and work  paid online...\n",
       "1549    throwback to one my client who reduced 38 kgs ...\n",
       "1550    taj mahal is beautiful today gymlife training ...\n",
       "Name: caption_text, Length: 1551, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caption_combined = pd.DataFrame({\n",
    "    \"cleaned_caption\": df_caption_cleaned,\n",
    "    \"tokens\": df_caption_cleaned_tokens\n",
    "})\n",
    "df_caption_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6d17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
